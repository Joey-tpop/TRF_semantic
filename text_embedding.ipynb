{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbgoZ6W9e36CGREvWBRo7P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joey-tpop/TRF_semantic/blob/main/text_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Joey-tpop/Semantic_dissimilarity.git\n",
        "%cd Semantic_dissimilarity\n",
        "\n",
        "!git branch\n",
        "!git checkout -b main\n",
        "!git add README.md\n",
        "!git commit -m \"Initial commit on main\"\n",
        "!git checkout -b main\n",
        "!git push -u origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYO_IET4oB0W",
        "outputId": "24b40612-e59a-4ff8-8337-5ea398d35846"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Semantic_dissimilarity'...\n",
            "warning: You appear to have cloned an empty repository.\n",
            "/content/Semantic_dissimilarity/Semantic_dissimilarity/Semantic_dissimilarity/Semantic_dissimilarity/Semantic_dissimilarity/Semantic_dissimilarity/Semantic_dissimilarity\n",
            "Switched to a new branch 'main'\n",
            "fatal: pathspec 'README.md' did not match any files\n",
            "On branch main\n",
            "\n",
            "Initial commit\n",
            "\n",
            "nothing to commit (create/copy files and use \"git add\" to track)\n",
            "Switched to a new branch 'main'\n",
            "error: src refspec main does not match any\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/Joey-tpop/Semantic_dissimilarity.git'\n",
            "\u001b[m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method 1: Transformer-based Sentence Embedding**\n",
        "\n",
        "- Great Performance when embedding context\n",
        "- Not very efficient in the Word Surprisal Measurement"
      ],
      "metadata": {
        "id": "mPMjiAZapCo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from scipy.spatial.distance import cosine\n",
        "from scipy.spatial.distance import euclidean\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "Qkc5BLWXzwVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"I didn't do my homework\", \"I didn't do my\"]\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "cos_similarity = util.cos_sim(embeddings[0], embeddings[1])\n",
        "l2_distance = euclidean(embeddings[0], embeddings[1])\n",
        "corr, _ = pearsonr(embeddings[0], embeddings[1])\n",
        "\n",
        "print(\"Cosine similarity:\", cos_similarity.item())\n",
        "print(\"l2 distance:\", l2_distance)\n",
        "print(\"pearson correlation: \", corr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSgg6AW62NzE",
        "outputId": "5e1589cd-b456-4d07-bf85-b036e60c8538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity: 0.5510249733924866\n",
            "l2 distance: 0.9476023316383362\n",
            "pearson correlation:  0.5507165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"I didn't do my banana\", \"I didn't do my\"]\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "cos_similarity = util.cos_sim(embeddings[0], embeddings[1])\n",
        "l2_distance = euclidean(embeddings[0], embeddings[1])\n",
        "corr, _ = pearsonr(embeddings[0], embeddings[1])\n",
        "\n",
        "print(\"Cosine similarity:\", cos_similarity.item())\n",
        "print(\"l2 distance:\", l2_distance)\n",
        "print(\"pearson correlation: \", corr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0KHvMj_Tc5b",
        "outputId": "61106d9c-273b-4738-9a52-d17fdf7a4041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity: 0.5493135452270508\n",
            "l2 distance: 0.9494067430496216\n",
            "pearson correlation:  0.5489815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method 2: logic-based Measurement**\n",
        "\n",
        "- Great Performance when embedding context\n",
        "- Also very efficient in the Word Surprisal Measurement"
      ],
      "metadata": {
        "id": "6hlx_Y1oVhYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. obtaining tokenizer(word -> vector) and model (next word prediction)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()  # training off\n",
        "\n",
        "# 2. sentence input\n",
        "sentence = \"I didn't do my\"\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\") # pt: python tensor\n",
        "\n",
        "# 3. sentence(text) input to the model and obtain the logits\n",
        "with torch.no_grad():  # gradient computation off (for faster operation)\n",
        "    outputs = model(**inputs)\n",
        "    # [batch_size, seq_len, vocab_size]\n",
        "    # the next prediction will be in the [batch_size, seq_len(=-1), :]\n",
        "    logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
        "\n",
        "# 4. Input the target word (the current word, where we measure the surprisal)\n",
        "target_words = [\"homework\", \"banana\", \"printer\", \"cup\", \"work\"]\n",
        "target_token_ids = [tokenizer.encode(word, add_special_tokens=False)[0] for word in target_words]\n",
        "\n",
        "# 5. Obtain the probability(and surprisal) of occurence of the target word\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "target_prob = probs[:, -1, target_token_ids]\n",
        "surprisal = -torch.log(target_prob)\n",
        "print(surprisal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dts7Mqw12iB8",
        "outputId": "0aafdaed-d622-4f9a-fe8c-57d84abbda29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[16.2866, 19.2577, 17.9780, 21.5103, 14.0107]])\n"
          ]
        }
      ]
    }
  ]
}